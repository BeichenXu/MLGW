{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices:[cuda(id=0), cuda(id=1), cuda(id=2)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import optax\n",
    "import flax\n",
    "from flax.training.train_state import TrainState\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "from dataset.toytext import TextDataset\n",
    "from models.language import BigramLM, TransormerLM, MambaLM\n",
    "import training\n",
    "\n",
    "print(f\"JAX devices:{jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 00:04:55.333072: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "rng_key = jax.random.key(0)\n",
    "max_context_len = 32\n",
    "batch_size = 64\n",
    "\n",
    "dataset = TextDataset(data_path=\"dataset/shakespeare.txt\")\n",
    "model = MambaLM(\n",
    "    vocab_size=len(dataset.tokenizer.vocab),\n",
    "    max_context_len=max_context_len,\n",
    "    embedding_dim=64,\n",
    "    state_dim=128,\n",
    "    n_layers=4\n",
    ")\n",
    "# model= TransormerLM(\n",
    "#     vocab_size=len(dataset.tokenizer.vocab),\n",
    "#     max_context_len=max_context_len,\n",
    "#     embedding_dim=64,\n",
    "#     head_size=128,\n",
    "#     n_heads=4,\n",
    "#     n_layers=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b9a81d625743e4904735a135f57a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7e6ea79cd04c4ca0688444285fc931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 00:05:04.311235: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to allocate 4461576 bytes for new constant\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Failed to allocate 4461576 bytes for new constant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_rng_key \u001b[38;5;129;01min\u001b[39;00m tqdm(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(epoch_rng_key, batches_per_epoch), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m get_batch(batch_size, max_context_len, rng_key\u001b[38;5;241m=\u001b[39mbatch_rng_key)\n\u001b[1;32m     27\u001b[0m     train_state, loss_value \u001b[38;5;241m=\u001b[39m optimization_step(train_state, x, y)\n\u001b[1;32m     28\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch_env_clone/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1159\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1157\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxla_executable\u001b[38;5;241m.\u001b[39mexecute_sharded(input_bufs)\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1161\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Failed to allocate 4461576 bytes for new constant"
     ]
    }
   ],
   "source": [
    "optimization_step = jax.jit(\n",
    "    partial(training.optimization_step, loss_fn=training.logit_prediction_loss)\n",
    ")\n",
    "get_batch = jax.jit(dataset.get_batch, static_argnames=[\"batch_size\", \"context_len\"])\n",
    "generate_token = jax.jit(partial(model.apply, method=model.generate_token))\n",
    "\n",
    "def generate_text(params, prompt: str, length=500, rng_key=jax.random.key(0)):\n",
    "    context = dataset.tokenizer.encode(prompt)\n",
    "    print(\"\\033[94m\", dataset.tokenizer.decode(context), \"\\033[0m\", end=\"\")\n",
    "    for sub_rng in jax.random.split(rng_key, length):\n",
    "        next_token, context = generate_token(params, context, sub_rng)\n",
    "        print(dataset.tokenizer.decode(next_token[None]), end=\"\")\n",
    "\n",
    "\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=model.init(rng_key, dataset.sample(max_context_len, rng_key)),\n",
    "    tx=optax.chain(optax.clip(1.0), optax.adam(3e-4, b2=0.95)),\n",
    ")\n",
    "\n",
    "N_epochs = 10\n",
    "batches_per_epoch = 1000\n",
    "for epoch_idx, epoch_rng_key in enumerate(tqdm(jax.random.split(rng_key, N_epochs))):\n",
    "    losses = []\n",
    "    for batch_rng_key in tqdm(jax.random.split(epoch_rng_key, batches_per_epoch), leave=False):\n",
    "        x, y = get_batch(batch_size, max_context_len, rng_key=batch_rng_key)\n",
    "        train_state, loss_value = optimization_step(train_state, x, y)\n",
    "        losses.append(loss_value)\n",
    "    print(f\"Loss: {sum(losses) / len(losses)}\\nGeneration test:\")\n",
    "    generate_text(train_state.params, prompt=dataset.fulltext[:max_context_len], rng_key=rng_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
